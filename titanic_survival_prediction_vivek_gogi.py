# -*- coding: utf-8 -*-
"""Titanic Survival Prediction - Vivek Gogi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iG5MgbGCAxbvdZqwVdlL0NIU_y8n1jV9

# **Importing Libraries & Dataset**

### **Importing Libraries:**
"""

import re
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from matplotlib import style
from sklearn.preprocessing import LabelEncoder
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

"""### **Importing Dataset:**"""

test_df = pd.read_csv("test.csv")
train_df = pd.read_csv("train.csv")

"""# **Exploratory Data Analysis**

### **Basic Observations:**
"""

train_df.head()

train_df.describe()

train_df.info()

train_df.isnull().sum()
sns.heatmap(train_df.isnull(), cbar = False).set_title("Missing Values Heatmap")

train_df.isnull().sum()

"""> **Conclusion**

- **Numerical** - Age, Fare, PassengerId
- **Categorical** - Survived, Pclass, Sex, SibSp, Parch, Embarked
- **Mixed** - Name, Ticket, Cabin

## **Univariate Data Analysis:**

### **Numerical Features - Age, Fare**

**1. Age**
"""

train_df['Age'].describe()

train_df['Age'].plot(kind='hist',bins=20)

train_df['Age'].isnull().sum()

"""**Conclusion:**

- The youngest traveler onboard was aged around two months and the oldest traveler was 80 years.
- The average age of tourists onboard was just under 30 years.
- In Age 20% of the values are missing.

**1. Fare**
"""

train_df['Fare'].describe()

train_df['Fare'].plot(kind='hist')

train_df['Fare'].isnull().sum()

"""**Conclusion:**

- Fare feature actually contains the group fare and not the individual fare (This migth be and issue).
- We need to create a new feature called individual_fare.

### **Categorical Features - Survived, Pclass, Sex, SibSp, Parch, Embarked**

**1. Survived**
"""

sns.countplot(data=train_df, x='Survived')

plt.xlabel('Survived')
plt.ylabel('Count')
plt.show()

death_percent = round((train_df['Survived'].value_counts().values[0]/891)*100)
survived_percent = round((train_df['Survived'].value_counts().values[1]/891)*100)

print("Died     : {}%".format(death_percent))
print("Survived : {}%".format(survived_percent))

"""**Conclusion:**

- Here, 62% people died in the accident.
- Here, 38% people survived in the accident.
- This feature does not have any missing values.

**2. Pclass**
"""

sns.countplot(data=train_df, x='Pclass')

plt.xlabel('Pclass')
plt.ylabel('Count')

"""**Conclusion:**

- Here, Most of the passengers were travelled from Class-3, after that Class-1, and Class-2.
- This feature does not have any missing values.

**3. Sex**
"""

sns.countplot(data=train_df, x='Sex')

plt.xlabel('Sex')
plt.ylabel('Count')
plt.show()

male = round((train_df['Sex'].value_counts().values[0]/891)*100)
female = round((train_df['Sex'].value_counts().values[1]/891)*100)

print("Male   :{}%".format(male))
print("Female :{}%".format(female))

"""**Conclusion:**

- Here, 65% passengers are male and 35% passengers are female.

**4. SibSp**
"""

sns.countplot(data=train_df, x='SibSp')

plt.xlabel('Sex')
plt.ylabel('Count')
plt.show()

print(train_df['SibSp'].value_counts())

"""**Conclusion:**

-  A maximum of 8 siblings and spouses traveled along with one of the traveler. More than 90% of people traveled alone or with one of their sibling or spouse.
- This feature does not have any missing values.

**5. Parch**
"""

sns.countplot(data=train_df, x='Parch')

plt.xlabel('Parch')
plt.ylabel('Count')
plt.show()

print((train_df['Parch'].value_counts()/891)*100)

"""**Conclusion:**

-  This feature contained the number of parents or children each passenger was touring with. A maximum of 76% passengers travelled with no parents or child.
- This feature does not have any missing values.

**6. Embarked**
"""

sns.countplot(data=train_df, x='Embarked')

plt.xlabel('Embarked')
plt.ylabel('Count')
plt.show()

print((train_df['Embarked'].value_counts()/891)*100)

train_df['Embarked'].isnull().sum()

"""**Conclusion:**

- Embarked implies where the traveler mounted from. There are three possible values for Embark â€” Southampton, Cherbourg, and Queenstown.

- More than 72% of the people boarded from Southampton.

- Just under 28% boarded from Cherbourg and the rest boarded from Queenstown.

- This feature is having 2 missing values.

## **Multivariate Data Analysis:**

**1. Pclass with Fare**
"""

plt.figure(figsize=(15, 6))
sns.kdeplot(train_df[train_df['Pclass'] == 1]['Fare'], label='Pclass-1')
sns.kdeplot(train_df[train_df['Pclass'] == 2]['Fare'], label='Pclass-2')
sns.kdeplot(train_df[train_df['Pclass'] == 3]['Fare'], label='Pclass-3')
plt.xlim(0)
plt.legend(loc='upper right')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='Pclass', y='Fare', data=train_df)
plt.ylim(0)
plt.xlabel('Passenger Class')
plt.ylabel('Fare')
plt.title('Distribution of Fares by Passenger Class')
plt.show()

"""**Conclusion:**
- From the above visuals it is proved that the Pclass-1 is more costlier, then Pclass-2, and Pclass-3.
- As the Outliers are present in the fare for the Pclass-1 and Pclass-2, But we should keep in mind that this is the fare of entire family, this is not an individual fare.
- To deal with this outliers i will create the new feature called individual_fare which resolves the overall problem.

**2. Survived with Pclass**
"""

sns.countplot(x=train_df['Survived'], hue=train_df['Pclass'])

"""**Conclusion:**

- The survival chances of a class-1 traveler were higher than a class-2 and class-3 traveler.

**3. Survived with Sex**
"""

sns.countplot(x=train_df['Survived'], hue=train_df['Sex'])

"""**Conclusion:**

- Approximately 65% of the passengers were male while the remaining 35% were female.
- Most of the male passengers died, as compared to female passengers.

**4. Survived with Embarked**
"""

sns.countplot(x=train_df['Survived'], hue=train_df['Embarked'])

"""**Conclusion:**

- Passengers who boarded from Cherbourg had a higher chance of survival than people who boarded from Southampton or Queenstown.

**5. Survived with Age**
"""

plt.figure(figsize=(15, 6))
sns.kdeplot(train_df[train_df['Survived'] == 0]['Age'], label='Not Survived')
sns.kdeplot(train_df[train_df['Survived'] == 1]['Age'], label='Survived')
plt.xlim(0)
plt.legend(loc='upper right')
plt.show()

"""**Conclusion:**

- Clearly, a larger fraction of children under 10 survived than died.
- Passengers belongs to age range 18 to 30 are not survived.
- Few Passenger of age above 80 are survived.

**6. Survived with Fare**
"""

plt.figure(figsize=(15, 6))
sns.kdeplot(train_df[train_df['Survived'] == 0]['Fare'], label='Not Survived')
sns.kdeplot(train_df[train_df['Survived'] == 1]['Fare'], label='Survived')
plt.xlim(0)
plt.legend(loc='upper right')
plt.show()

"""**Conclusion:**
- It was obvious that there was a strong association between the charge and the survival. The higher a passengers paid, the higher would be his chances to survive.

**7. Survived with Age and Sex**
"""

survived = 'survived'
not_survived = 'not survived'
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))

for gender, ax in zip(['female', 'male'], axes):
    data = train_df[train_df['Sex'] == gender]

    ax = sns.distplot(data[data['Survived'] == 1].Age.dropna(), bins=18, label=survived, ax=ax, kde=False)
    ax = sns.distplot(data[data['Survived'] == 0].Age.dropna(), bins=40, label=not_survived, ax=ax, kde=False)
    ax.legend()
    ax.set_title('Female' if gender == 'female' else 'Male')

"""**Conclusion:**

- Men between 18 and 30 have a good chance of survival, although women also have some likelihood but not as much. Women have higher survival chances between 14 and 40.

- Men have a low chance of survival between ages 5 and 18, but this doesn't apply to women. Additionally, infants also have slightly better survival odds.

**8. Survived with Embarked, Pclass, Sex**
"""

sns.catplot(data=train_df, x='Pclass', y='Survived', hue='Sex', kind='point', col='Embarked', height=4.5, aspect=1.6, palette=None, order=None, hue_order=None)

"""**Conclusion:**

- Embarked seems to be correlated with survival, depending on the gender.
- Women on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C.
-Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S.

**8. Survived with Embarked, Pclass, Sex**
"""

fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 10))

for i, survived in enumerate([0, 1]):
    for j, pclass in enumerate([1, 2, 3]):
        data = train_df[(train_df['Survived'] == survived) & (train_df['Pclass'] == pclass)]
        axes[j, i].hist(data['Age'], alpha=0.5, bins=20)
        axes[j, i].set_title(f'Survived={survived}, Pclass={pclass}')
        axes[j, i].set_xlabel('Age')
        axes[j, i].set_ylabel('Frequency')

fig.legend(['Not Survived', 'Survived'], loc='center right')
fig.tight_layout()
plt.show()

"""**Conclusion:**

- The plot above validates our initial assumption regarding pclass 1, supporting the notion that individuals in this class have a higher likelihood of survival.
- Additionally, it reveals a notable probability that individuals in pclass 3 will not survive.

# **Data Preprocessing**

### **Handling Missing Values:**

Now I will handle the issue of missing values in the following features: **Age-177**, **Cabin-687**, **Embarked-2**

> **1. Age**

Now let's handle the issue of missing values in the age feature. I will create an array that contains random numbers, which are computed based on the mean age value in regards to the standard deviation and is_null.

The advantage of this method is that it fills the missing values in the "Age" column with random numbers generated based on the mean, standard deviation, and number of missing values.
"""

data = [train_df, test_df]

for dataset in data:
    mean = train_df["Age"].mean()
    std = test_df["Age"].std()
    is_null = dataset["Age"].isnull().sum()
    rand_age = np.random.randint(mean - std, mean + std, size = is_null)
    age_slice = dataset["Age"].copy()
    age_slice[np.isnan(age_slice)] = rand_age
    dataset["Age"] = age_slice
    dataset["Age"] = train_df["Age"].astype(int)

print("Count Of Missing Values - Age : ", train_df["Age"].isnull().sum())

"""Let's visualize the **Survival** feature with the **Age** feature."""

plt.figure(figsize=(15, 6))
sns.kdeplot(train_df[train_df['Survived'] == 0]['Age'], label='Not Survived')
sns.kdeplot(train_df[train_df['Survived'] == 1]['Age'], label='Survived')
plt.xlim(0)
plt.legend(loc='upper right')
plt.show()

"""This approach introduces some randomness to the imputation process, which can help to better preserve the statistical properties of the age distribution.

> **2. Cabin**

Initially, I considered removing the 'Cabin' feature, but then I discovered something interesting. The cabin number follows a pattern like 'C123', where the letter represents the deck. So, we will extract this information and create a new feature to indicate the deck where a person's cabin was located.

Any missing values will be replaced with U. The picture below shows the decks of the Titanic, labeled from A to G.
"""

data = [train_df, test_df]

for dataset in data:
    dataset['Cabin'] = dataset['Cabin'].fillna('U0')
    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile("([a-zA-Z]+)").search(x).group())

train_df = train_df.drop(['Cabin'], axis=1)
test_df = test_df.drop(['Cabin'], axis=1)

print("Count Of Missing Values - Deck : ", train_df["Deck"].isnull().sum())

"""Let's visualize the **Survival** feature with the **Deck** feature."""

sns.countplot(x=train_df['Survived'], hue=train_df['Deck'])

"""> **3. Embarked**

Since there are only 2 missing values in the Embarked feature, we can simply fill them with the most common value.
"""

train_df['Embarked'].describe()

common_value = 'S'
data = [train_df, test_df]

for dataset in data:
    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)

print("Count Of Missing Values - Embarked : ", train_df["Embarked"].isnull().sum())

"""Let's visualize the **Survival** feature with the **Embarked** feature."""

sns.countplot(x=train_df['Survived'], hue=train_df['Embarked'])

"""### **Removing Features:**

> **PassengerId**

I will remove the 'PassengerId' from the training set because it does not contribute to a person's survival probability. However, I will keep it in the test set since it is required for submission purposes.
"""

train_df = train_df.drop(['PassengerId'], axis=1)

"""> **Ticket**

I will remove the 'Ticket' from both training set and testing set because it does not contribute to a person's survival probability.
"""

train_df = train_df.drop(['Ticket'], axis=1)
test_df = test_df.drop(['Ticket'], axis=1)

"""# **Feature Engineering**

### **Creating New Features:**

I will add few new features to the dataset, that I compute out of other features.

> **1. Title**

For the 'Name' feature, we will extract the titles from the names in order to create a new feature based on that information.
"""

data = [train_df, test_df]
# titles = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5}

for dataset in data:
    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)
    dataset["Title"].value_counts()


for dataset in data:
    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\
                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')
    dataset["Title"].value_counts()

train_df = train_df.drop(['Name'], axis=1)
test_df = test_df.drop(['Name'], axis=1)

"""Let's visualize the **Survival** feature with the **Title** feature."""

sns.countplot(x=train_df['Survived'], hue=train_df['Title'])

"""Hence, From the above visual we can conclude that survival probability of a passenger also depends on the initial of passengers name.

> **2. Family Size**
"""

data = [train_df, test_df]
for dataset in data:
    dataset['family_members'] = dataset['SibSp'] + dataset['Parch'] + 1
    dataset.loc[dataset["family_members"] == 1, "family_size"] = 'singleton'
    dataset.loc[(dataset["family_members"] > 1)  &  (dataset["family_members"] < 5) , "family_size"] = 'small'
    dataset.loc[dataset["family_members"] > 4, "family_size"] = 'large'

print(train_df["family_members"].value_counts())

"""Let's visualize the **Survival** feature with the **family_size** feature."""

sns.countplot(x=train_df['Survived'], hue=train_df['family_size'])

"""Hence, From the above visual we can conclude that survival probability of a passenger also depends on the passengers family size.

Passengers having small families will have the higher chance of survival.

> **3. Individual Fare**
"""

for dataset in data:
    dataset['individual_fare'] = dataset['Fare'] / dataset['family_members']
    dataset['individual_fare'] = dataset['individual_fare'].fillna(0)
    dataset['individual_fare'] = dataset['individual_fare'].replace([np.inf, -np.inf], 0)
    dataset['individual_fare'] = dataset['individual_fare'].astype(int)

"""Let's visualize the **Survival** feature with the **individual_fare** feature."""

plt.figure(figsize=(15, 6))
sns.kdeplot(train_df[train_df['Survived'] == 0]['individual_fare'], label='Not Survived')
sns.kdeplot(train_df[train_df['Survived'] == 1]['individual_fare'], label='Survived')
plt.xlim(0)
plt.legend(loc='upper right')
plt.show()

train_df['individual_fare'] < 0

"""Hence, From the above visuals we conclude that, The higher a passengers paid, the higher would be his chances to survive.

### **Converting Features:**

Here, We have to deal with 5 categorical columns such as **Sex**, **Embarked**, **Deck**, **Title**, **family_size** Lets investigate and transfrom them.
"""

data = [train_df, test_df]
columns_to_encode = ['Sex', 'Embarked', 'Deck', 'Title', 'family_size']
label_encoder = LabelEncoder()

for dataset in data:
    for column in columns_to_encode:
        dataset[column] = label_encoder.fit_transform(dataset[column])

"""**Let's take a last look at the training set, before we start training the models.**"""

train_df.head()

"""# **Model Training**

### **Dataset Splitting:**

Now, I will train multiple Machine Learning models and compare their results. Since the dataset does not provide labels for the testing set, we will use the entire train_df and split this into X_train, X_test, y_train, y_test respectively.
"""

X = train_df.drop("Survived", axis=1)
y = train_df["Survived"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""### **Logistic Regression:**"""

lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

"""### **K Nearest Neighbor:**"""

knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

"""### **Linear Support Vector Machine:**"""

svc = LinearSVC()
svc.fit(X_train, y_train)
y_pred_svc = svc.predict(X_test)

"""### **Decision Tree:**"""

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

"""### **Random Forest:**"""

rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

"""### **XGBoost Classifier:**"""

xgb_classifier = XGBClassifier()
xgb_classifier.fit(X_train, y_train)
y_pred_xgb = xgb_classifier.predict(X_test)

"""### **Comparing Models:**

Now, I will compare the different models based on their Evaluation Metrics.

> **Confusion Matrix**
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

models = {
    'Logistic Regression': (y_pred_lr, confusion_matrix(y_test, y_pred_lr)),
    'KNN': (y_pred_knn, confusion_matrix(y_test, y_pred_knn)),
    'SVM': (y_pred_svc, confusion_matrix(y_test, y_pred_svc)),
    'Decision Tree': (y_pred_dt, confusion_matrix(y_test, y_pred_dt)),
    'Random Forest': (y_pred_rf, confusion_matrix(y_test, y_pred_rf)),
    'XGBoost Classifier': (y_pred_xgb, confusion_matrix(y_test, y_pred_xgb)),
}

print("Confusion Matrices - \n")

for model_name, (y_pred, confusion_matrix) in models.items():
    print(f"{model_name}:")
    print(confusion_matrix)
    print()

"""> **Accuracy Score, Precision Score, Recall Score, F1-Score**"""

models = ['Logistic Regression', 'K Nearest Neighbor', 'Linear SVM', 'Decision Tree', 'Random Forest', 'XGBoost Classifier']
predictions = [y_pred_lr, y_pred_knn, y_pred_svc, y_pred_dt, y_pred_rf, y_pred_xgb]

metrics = {
    'Model': models,
    'Accuracy': [accuracy_score(y_test, pred) for pred in predictions],
    'Precision': [precision_score(y_test, pred) for pred in predictions],
    'Recall': [recall_score(y_test, pred) for pred in predictions],
    'F1 Score': [f1_score(y_test, pred) for pred in predictions]
}

metrics_df = pd.DataFrame(metrics)

print(metrics_df)

"""### **Best Model:**

Based on the current results, the **Random Forest Classifier** is performing well.
"""

print("Random Forest Classifier:")
print("Accuracy   :", accuracy_score(y_test, y_pred_rf),)
print("Precision  :", precision_score(y_test, y_pred_rf))
print("Recall     :", recall_score(y_test, y_pred_rf))
print("F1 Score   :", f1_score(y_test, y_pred_rf))

"""# **Hyperparameter Tunning**

### **Model Tunning:**
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_classification

param_grid = {
    'criterion' : ["gini", "entropy"],
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

rf_classifier = RandomForestClassifier()

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

"""### **Best Parameters:**"""

print("Best Parameters: ", grid_search.best_params_)
print("Best Score: ", grid_search.best_score_)

"""### **Model Training**"""

rf_clf_titanic = RandomForestClassifier(criterion = "gini",
                                       max_depth = None,
                                       min_samples_split = 10,
                                       n_estimators = 200)

rf_clf_titanic.fit(X_train, y_train)
y_pred_final = rf_clf_titanic.predict(X_test)

"""# **Model Evaluation**"""

print("Random Forest Classifier:")
print("Accuracy   :", accuracy_score(y_test, y_pred_final),)
print("Precision  :", precision_score(y_test, y_pred_final))
print("Recall     :", recall_score(y_test, y_pred_final))
print("F1 Score   :", f1_score(y_test, y_pred_final))

"""# **Pickling ML Model**"""

import pickle

with open('rf_clf_titanic.pkl', 'wb') as file:
    pickle.dump(rf_clf_titanic, file)